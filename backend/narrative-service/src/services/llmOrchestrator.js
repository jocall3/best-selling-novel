import { OpenAI } from 'openai';
import config from '../../config/config'; // Adjust path as per your project's config location

/**
 * @class LLMOrchestrator
 * @description Manages all interactions with external Large Language Models (LLMs)
 *              for content generation, synthesis, and other text manipulation tasks.
 */
class LLMOrchestrator {
    constructor() {
        this.openai = null;
        this.defaultModel = config.llm.defaultModel || 'gpt-4o-mini'; // Default model, can be overridden by config
        this.temperature = config.llm.temperature !== undefined ? config.llm.temperature : 0.7; // Default creativity level

        this._initializeLLMProviders();
    }

    /**
     * Initializes connections to various LLM providers based on configuration.
     * Currently supports OpenAI.
     * @private
     */
    _initializeLLMProviders() {
        if (config.llm.openaiApiKey) {
            this.openai = new OpenAI({
                apiKey: config.llm.openaiApiKey,
            });
            console.log('OpenAI service initialized.');
        } else {
            console.warn('OpenAI API key not found in configuration. OpenAI service will not be available.');
        }

        // Future: Add other LLM providers here (e.g., Anthropic, Cohere, local models)
        // if (config.llm.anthropicApiKey) {
        //     this.anthropic = new Anthropic({ apiKey: config.llm.anthropicApiKey });
        //     console.log('Anthropic service initialized.');
        // }
    }

    /**
     * Makes a raw API call to the specified LLM provider.
     * This private method abstracts the actual API interaction.
     * @param {string} provider - The name of the LLM provider (e.g., 'openai').
     * @param {string} model - The specific model to use (e.g., 'gpt-4o-mini').
     * @param {Array<Object>} messages - The conversation history in the format expected by the LLM.
     * @param {Object} options - Additional LLM specific options (e.g., temperature, max_tokens).
     * @returns {Promise<string>} The content generated by the LLM.
     * @private
     */
    async _callLLM(provider, model, messages, options) {
        try {
            switch (provider) {
                case 'openai':
                    if (!this.openai) {
                        throw new Error('OpenAI service is not initialized.');
                    }
                    const completion = await this.openai.chat.completions.create({
                        model: model,
                        messages: messages,
                        temperature: options.temperature !== undefined ? options.temperature : this.temperature,
                        max_tokens: options.maxTokens || 2000, // Default max tokens, can be overridden
                        // Add other OpenAI specific options like top_p, frequency_penalty etc.
                        ...options.llmSpecific || {}
                    });
                    return completion.choices[0].message.content;

                // Future: Add cases for other providers
                // case 'anthropic':
                //     // Call Anthropic API
                //     break;

                default:
                    throw new Error(`Unsupported LLM provider: ${provider}`);
            }
        } catch (error) {
            console.error(`Error calling LLM provider "${provider}":`, error);
            if (error.response) {
                console.error('LLM API Error Response:', error.response.data);
            }
            throw new Error(`LLM API call failed for ${provider} with model ${model}: ${error.message}`);
        }
    }

    /**
     * Generates textual content based on a system prompt and a user prompt.
     * This is the primary method for generating new text.
     * @param {string} systemPrompt - The overarching instructions for the LLM's role and behavior.
     * @param {string} userPrompt - The specific task or question for the LLM to address.
     * @param {object} [options={}] - Optional parameters for the LLM call.
     * @param {string} [options.model] - The LLM model to use (defaults to this.defaultModel).
     * @param {number} [options.temperature] - Controls randomness (0-2). Lower values mean more deterministic output.
     * @param {number} [options.maxTokens] - The maximum number of tokens to generate.
     * @param {string} [options.provider='openai'] - The LLM provider to use.
     * @returns {Promise<string>} The generated content.
     */
    async generateContent(systemPrompt, userPrompt, options = {}) {
        const provider = options.provider || 'openai'; // Default to OpenAI
        const model = options.model || this.defaultModel;

        const messages = [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt },
        ];

        return this._callLLM(provider, model, messages, options);
    }

    /**
     * Synthesizes or summarizes an array of content chunks into a cohesive piece.
     * This is useful for combining multiple generated sections, summarizing long texts,
     * or creating an outline from disparate points.
     * @param {string[]} contentChunks - An array of strings, each representing a piece of content to synthesize.
     * @param {string} synthesisInstruction - Specific instructions for the synthesis
     *                                      (e.g., "Summarize the following points",
     *                                      "Combine these sections into a cohesive paragraph",
     *                                      "Create an executive summary from the detailed reports below").
     * @param {object} [options={}] - Optional parameters for the LLM call.
     * @param {number} [options.temperature=0.5] - Defaults to a lower temperature for more factual/concise output.
     * @returns {Promise<string>} The synthesized content.
     */
    async synthesizeContent(contentChunks, synthesisInstruction, options = {}) {
        if (!Array.isArray(contentChunks) || contentChunks.length === 0) {
            throw new Error('Content chunks must be a non-empty array for synthesis.');
        }

        const combinedContent = contentChunks.join('\n\n---\n\n'); // Use a clear separator
        const userPrompt = `${synthesisInstruction}\n\nHere is the content to synthesize:\n\n${combinedContent}`;

        // For synthesis, we typically want less creativity, more coherence/accuracy
        const synthesisOptions = {
            ...options,
            temperature: options.temperature !== undefined ? options.temperature : 0.5,
            maxTokens: options.maxTokens || 3000, // Allow more tokens for comprehensive synthesis
        };

        const systemPrompt = "You are an expert editor, summarizer, and content synthesizer. Your task is to accurately and cohesively process and integrate the provided information according to the given instructions. Focus on clarity, conciseness, and maintaining the core meaning.";

        return this.generateContent(systemPrompt, userPrompt, synthesisOptions);
    }

    /**
     * Refines a given piece of text based on specific instructions.
     * Useful for improving grammar, style, tone, or coherence.
     * @param {string} text - The original text to refine.
     * @param {string} refinementInstruction - Instructions on how to refine the text
     *                                         (e.g., "Improve grammar and flow", "Make it sound more professional",
     *                                         "Shorten this paragraph by 20% while retaining key information").
     * @param {object} [options={}] - Optional parameters for the LLM call.
     * @returns {Promise<string>} The refined text.
     */
    async refineText(text, refinementInstruction, options = {}) {
        if (!text || typeof text !== 'string') {
            throw new Error('Text to refine must be a non-empty string.');
        }

        const systemPrompt = "You are a meticulous editor and writer. Your goal is to refine the provided text based on the given instructions, focusing on grammar, clarity, style, and conciseness without altering the core meaning unless specified.";
        const userPrompt = `${refinementInstruction}\n\nHere is the text to refine:\n\n${text}`;

        const refinementOptions = {
            ...options,
            temperature: options.temperature !== undefined ? options.temperature : 0.3, // Lower temp for refinement
            maxTokens: options.maxTokens || 2000, // Allow enough tokens for the refined output
        };

        return this.generateContent(systemPrompt, userPrompt, refinementOptions);
    }

    /**
     * Generates a list of ideas or concepts based on a given topic and specific requirements.
     * @param {string} topic - The topic for which to brainstorm ideas.
     * @param {string} brainstormInstruction - Specific instructions for brainstorming
     *                                        (e.g., "List 5 unique chapter ideas",
     *                                        "Suggest 10 different angles for a persuasive essay",
     *                                        "Brainstorm potential conflicts for a character").
     * @param {object} [options={}] - Optional parameters for the LLM call.
     * @returns {Promise<string>} A string containing the brainstormed ideas.
     */
    async brainstormIdeas(topic, brainstormInstruction, options = {}) {
        const systemPrompt = "You are a creative brainstorming assistant. Generate a diverse and innovative set of ideas based on the user's topic and instructions. Think broadly and provide distinct concepts.";
        const userPrompt = `Topic: "${topic}"\n\nInstructions: ${brainstormInstruction}`;

        const brainstormOptions = {
            ...options,
            temperature: options.temperature !== undefined ? options.temperature : 0.9, // Higher temp for creativity
            maxTokens: options.maxTokens || 1000,
        };

        return this.generateContent(systemPrompt, userPrompt, brainstormOptions);
    }
}

// Export a singleton instance of the LLMOrchestrator
export default new LLMOrchestrator();